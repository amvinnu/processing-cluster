#!/bin/bash

set -x

echo "starting to configure"

env

source /root/hadoop_files/configure_hadoop.sh

IP=$(ip -o -4 addr list eth0 | perl -n -e 'if (m{inet\s([\d\.]+)\/\d+\s}xms) { print $1 }')
echo "MASTER_IP=$IP"

echo "preparing Hadoop"
if [ -z "$MASTER_HOST" ]; then
  echo "Defining MASTER_HOST is mandatory !"
  exit 1
fi

prepare_hadoop $MASTER_HOST

echo "starting Hadoop Master services"
su - hdfs -c "/usr/bin/hdfs namenode -format"
service hadoop-hdfs-namenode start 

su - hdfs -c "hadoop fs -chown hdfs:hadoop /"
su - hdfs -c "hadoop fs -chmod 775 /"
su - hdfs -c "hadoop fs -mkdir /tmp"
su - hdfs -c "hadoop fs -chown mapred:hadoop /tmp"

service hadoop-yarn-resourcemanager start 
service hadoop-mapreduce-historyserver start 
service zookeeper-server start

su - hdfs -c "hadoop fs -mkdir -p hdfs:///user/spark/applicationHistory"
su - hdfs -c "hadoop fs -chmod 775 /user"
su - hdfs -c "hadoop fs -chown -R spark:spark /user/spark"

service spark-history-server start 

echo "starting sshd"
/usr/sbin/sshd

sleep 5

echo "starting Hadoop Master"
cp /root/hadoop_master_files/run_hadoop_master.sh /
chmod a+rx /run_hadoop_master.sh
/run_hadoop_master.sh
